{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec8e8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Question 0...\n",
      "\n",
      "{\n",
      "    \"is_correct\": true,\n",
      "    \"reasoning\": \"The AI-generated answer not only covers the core information from the ground answer but significantly expands upon it with valuable additional details. It correctly identifies SendGrid as the email service offered by Cloud Engineering and mentions the need for an API key (through the sub-user request process). The answer goes beyond the basic response by providing:\n",
      "    1. Clear organization with sections\n",
      "    2. Availability information\n",
      "    3. Specific features of the service\n",
      "    4. Detailed steps for getting started\n",
      "    5. Relevant links to documentation and the request form\n",
      "    All information provided aligns with the context and enhances the basic ground answer without contradicting it.\",\n",
      "    \"metrics\": {\n",
      "        \"relevance\": 1.0,\n",
      "        \"completeness\": 0.95,\n",
      "        \"accuracy\": 1.0\n",
      "    },\n",
      "    \"explanation_of_metrics\": {\n",
      "        \"relevance\": \"Perfect score as all provided information directly addresses the question about email services\",\n",
      "        \"completeness\": \"Nearly perfect score - covers all essential aspects plus additional useful information. Only slight deduction because it could have mentioned estimated time for API key provision\",\n",
      "        \"accuracy\": \"Perfect score as all technical details, features, and process steps match the provided context without any errors\"\n",
      "    }\n",
      "}\n",
      "Response for Question 0 written to nova_pro_optimized_results.txt\n",
      "\n",
      "Evaluating Question 1...\n",
      "\n",
      "{\n",
      "    \"is_correct\": true,\n",
      "    \"reasoning\": \"The AI-generated answer not only includes the core information from the ground answer (the ServiceNow ticket link) but also provides a more comprehensive response with detailed steps and additional context. The answer includes important prerequisites (reading documentation), specific form-filling instructions, post-submission process details, and contact information for support. All information provided is accurate according to the context and expands upon the ground answer in a helpful way. The inclusion of the actual steps involved in the process makes it more actionable for the user.\",\n",
      "    \"metrics\": {\n",
      "        \"relevance\": 1.0,\n",
      "        \"completeness\": 0.95,\n",
      "        \"accuracy\": 1.0\n",
      "    },\n",
      "    \"explanation_of_metrics\": {\n",
      "        \"relevance\": \"The answer directly addresses the user's need for TFE project access with practical steps\",\n",
      "        \"completeness\": \"Covers all major aspects including prerequisites, process steps, and follow-up support. Scored 0.95 instead of 1.0 because it could have mentioned the approval timeline\",\n",
      "        \"accuracy\": \"All provided information aligns perfectly with the context and ground truth, with no incorrect information\"\n",
      "    }\n",
      "}\n",
      "Response for Question 1 written to nova_pro_optimized_results.txt\n",
      "\n",
      "Evaluating Question 2...\n",
      "\n",
      "{\n",
      "    \"is_correct\": true,\n",
      "    \"reasoning\": \"The AI-generated answer not only correctly lists all the supported AWS services from the ground answer (EC2, SQL Server (IaaS), RDS, EFS/FSX, S3) but also provides additional valuable information:\\n1. It includes the backup types supported for each service\\n2. It mentions DynamoDB as a roadmap item\\n3. It provides relevant source URLs and ServiceNow form links\\n4. The answer is well-structured with clear formatting and categorization\\n5. All services mentioned align perfectly with the context provided from the SharePoint documentation\",\n",
      "    \"metrics\": {\n",
      "        \"relevance\": 1.0,\n",
      "        \"completeness\": 1.0,\n",
      "        \"accuracy\": 1.0\n",
      "    },\n",
      "    \"explanation_of_metrics\": {\n",
      "        \"relevance\": \"Perfect score as all information is directly related to AWS supported services\",\n",
      "        \"completeness\": \"Perfect score as it includes all services from ground answer plus additional useful context\",\n",
      "        \"accuracy\": \"Perfect score as all listed services and their capabilities match exactly with the source documentation\"\n",
      "    }\n",
      "}\n",
      "Response for Question 2 written to nova_pro_optimized_results.txt\n",
      "\n",
      "Evaluating Question 3...\n",
      "\n",
      "{\n",
      "    \"is_correct\": true,\n",
      "    \"reasoning\": \"The AI-generated answer not only covered all the services mentioned in the ground answer but also provided additional valuable information: 1) It included specific timeline details for each service, 2) It provided more Azure services that will be supported later (Azure Blob), 3) It properly categorized the information with bullet points and included source links, 4) It correctly identified Azure PostgreSQL's end of 2024 availability, and Cosmos DB and Azure MySQL being on the roadmap. The answer is actually more comprehensive than the ground answer while maintaining accuracy.\",\n",
      "    \"metrics\": {\n",
      "        \"relevance\": 1.0,\n",
      "        \"completeness\": 1.0,\n",
      "        \"accuracy\": 1.0\n",
      "    },\n",
      "    \"explanation_of_metrics\": {\n",
      "        \"relevance\": \"The answer directly addresses the question about future Azure service support and provides specific timelines\",\n",
      "        \"completeness\": \"The answer includes all services from the ground answer plus additional relevant services and specific dates from the context\",\n",
      "        \"accuracy\": \"All information provided matches exactly with both the ground answer and the context provided, with precise timeline details\"\n",
      "    }\n",
      "}\n",
      "Response for Question 3 written to nova_pro_optimized_results.txt\n",
      "\n",
      "Evaluating Question 4...\n",
      "\n",
      "{\n",
      "    \"is_correct\": false,\n",
      "    \"reasoning\": \"The AI-generated answer and the ground answer are completely different. The AI answer focuses on describing the resource group layout and infrastructure components of BaaS for dedicated subscriptions, while the ground answer specifically describes the backup schedule and retention policies (transaction log backups, regular backups, and weekly backups across different tiers). It seems that either the context provided is incomplete or the AI was given a different interpretation of the question. The generated answer, while detailed about the architecture, doesn't address the backup schedules and retention periods that were specified in the ground answer.\",\n",
      "    \"metrics\": {\n",
      "        \"relevance\": 0.3,\n",
      "        \"completeness\": 0.2,\n",
      "        \"accuracy\": 0.0\n",
      "    }\n",
      "}\n",
      "\n",
      "Note: \n",
      "- Relevance is scored 0.3 because while the answer discusses BaaS architecture, it doesn't address the specific backup schedules that appear to be the intended focus.\n",
      "- Completeness is scored 0.2 because while it provides detailed information about resource groups and infrastructure, it completely misses the backup schedule and retention information.\n",
      "- Accuracy is scored 0.0 because the information provided, while possibly accurate in its own right, doesn't match the ground answer at all and appears to be answering a different aspect of BaaS architecture.\n",
      "\n",
      "It appears there might be a disconnect between the context provided and the ground answer, as the context doesn't seem to contain the information about backup schedules and retention periods mentioned in the ground answer.\n",
      "Response for Question 4 written to nova_pro_optimized_results.txt\n",
      "\n",
      "Evaluating Question 5...\n",
      "\n",
      "{\n",
      "    \"is_correct\": false,\n",
      "    \"reasoning\": \"The AI-generated answer is incorrect as it states 'No relevant RACI information was found' when the ground truth clearly shows there is a detailed RACI matrix for the BaaS service. The ground truth provides specific role assignments for different teams including BaaS Platform Team (R), D&T Product Owner (R/I), Business Product Owner (C/I), and GCSO (C/I). The AI response missed this information entirely, though it did correctly structure its response to indicate the sources it checked. The provided context indeed doesn't contain the RACI information, which suggests this might be a limitation of the search/retrieval system rather than the AI's interpretation.\",\n",
      "    \"metrics\": {\n",
      "        \"relevance\": 0.3,\n",
      "        \"completeness\": 0.0,\n",
      "        \"accuracy\": 0.0\n",
      "    }\n",
      "}\n",
      "\n",
      "Explanation of metrics:\n",
      "- Relevance: 0.3 - While the answer acknowledges the question about RACI and provides a structured response about the search results, it fails to provide the actual RACI information that exists.\n",
      "- Completeness: 0.0 - The answer completely misses all the RACI details that are present in the ground truth, including team responsibilities and their R/A/C/I assignments.\n",
      "- Accuracy: 0.0 - The answer is fundamentally incorrect by stating no RACI information exists when it does, making it completely inaccurate compared to the ground truth.\n",
      "Response for Question 5 written to nova_pro_optimized_results.txt\n",
      "\n",
      "Evaluating Question 6...\n",
      "\n",
      "{\n",
      "    \"is_correct\": false,\n",
      "    \"reasoning\": \"The AI-generated answer fails to provide the actual SLA information that is clearly available in the context. Instead of stating the specific SLA times for different tiers in both production and non-production applications, it only provides a form link and disclaims that SLA details are not explicitly mentioned. This is incorrect as the context actually includes a detailed SLA table showing the acknowledgment times for different tiers. The ground answer correctly specifies that Production apps range from 1 hour (Tier 1) to 1+ days (Tiers 3-4), and non-production apps range from 1 business day (Tier 1) to 1+ business days (Tiers 2-4).\",\n",
      "    \"metrics\": {\n",
      "        \"relevance\": 0.4,\n",
      "        \"completeness\": 0.2,\n",
      "        \"accuracy\": 0.3\n",
      "    },\n",
      "    \"explanation_of_metrics\": {\n",
      "        \"relevance\": \"Gets partial score for providing the correct form link for restoration requests\",\n",
      "        \"completeness\": \"Very low score as it misses the core SLA information that was available in the context\",\n",
      "        \"accuracy\": \"Low score because it incorrectly states that SLA details are not explicitly mentioned when they are clearly present in the context\"\n",
      "    }\n",
      "}\n",
      "Response for Question 6 written to nova_pro_optimized_results.txt\n",
      "\n",
      "Evaluating Question 7...\n",
      "\n",
      "{\n",
      "    \"is_correct\": false,\n",
      "    \"reasoning\": \"While the AI-generated answer provides some useful information about tagging changes, it differs significantly from the ground answer in several important aspects:\\n\\n1. The ground answer specifically mentions the revision number (4.5) and date (May 15, 2025), which is missing in the AI answer.\\n\\n2. The ground answer emphasizes that the four tags must EXACTLY match their corresponding values in the APM platform - this critical requirement is not clearly stated in the AI answer.\\n\\n3. The AI answer includes additional information about retired and unchanged tags that isn't present in the ground answer and can't be verified from the given context.\\n\\n4. The AI answer includes several links and additional information about 'What's Next' that, while potentially useful, goes beyond the scope of the specific question about latest changes.\\n\\n5. The AI answer misses the core purpose of the update, which was to realign cloud tagging requirements with APM data.\",\n",
      "    \"metrics\": {\n",
      "        \"relevance\": 0.7,\n",
      "        \"completeness\": 0.6,\n",
      "        \"accuracy\": 0.5\n",
      "    }\n",
      "}\n",
      "Response for Question 7 written to nova_pro_optimized_results.txt\n",
      "\n",
      "Evaluating Question 8...\n",
      "\n",
      "{\n",
      "    \"is_correct\": false,\n",
      "    \"reasoning\": \"While the AI-generated answer provides some accurate information about CloudHealth, it misses several key points from the ground answer and includes some information not present in the provided context. Specifically, it fails to mention: 1) There is no cost for teams to use CloudHealth 2) The limitation regarding amortized cost display and the FinOps team's recommendation 3) The specific cloud platforms supported (Azure, AWS, GCP) 4) The limitation regarding shared Azure reallocation and AWS allocated fee costs. The AI answer focuses more on providing links and resources, which while helpful, weren't part of the core information in the ground answer.\",\n",
      "    \"metrics\": {\n",
      "        \"relevance\": 0.8,\n",
      "        \"completeness\": 0.5,\n",
      "        \"accuracy\": 0.7\n",
      "    },\n",
      "    \"explanation_of_metrics\": {\n",
      "        \"relevance\": \"0.8 because the information provided is relevant to CloudHealth, but includes unnecessary resource links\",\n",
      "        \"completeness\": \"0.5 because it misses about half of the important capabilities and limitations mentioned in the ground answer\",\n",
      "        \"accuracy\": \"0.7 because the information provided is accurate but incomplete compared to the ground answer\"\n",
      "    }\n",
      "}\n",
      "Response for Question 8 written to nova_pro_optimized_results.txt\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from crewai import LLM\n",
    "import pandas as pd\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "llm = LLM(model=\"bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0\")\n",
    "\n",
    "clean = pd.read_excel(\"nova_pro_optimized_generated_data.xlsx\")\n",
    "# Example cloud-related questions\n",
    "questions = clean[\"user_input\"].tolist()\n",
    "ground_answers = clean[\"ground_truth\"].tolist()\n",
    "generated_answers = clean[\"ai_response\"].tolist()\n",
    "contexts = clean[\"cleaned_output\"].tolist()\n",
    "l = len(questions)\n",
    "# Specify the filename\n",
    "filename = \"nova_pro_optimized_results.txt\"\n",
    "with open(filename, \"w\") as f:\n",
    "    pass # No operation needed as just clearing the file\n",
    "\n",
    "\n",
    "# Judge prompt template\n",
    "def create_judge_prompt(question, answer_a, answer_b, context):\n",
    "    return f\"\"\"\n",
    "You are an expert in validating the generated answers based on ground answer and contexts provided to the agent based on the data from the vector database. \n",
    "Given the following question and their answers, validate the answer and its completeness.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Ground Answer: {answer_a}\n",
    "\n",
    "AI Generated Answer: {answer_b}\n",
    "\n",
    "Context from tool provided to the agent: {context}\n",
    "\n",
    "Your task is to evaluate the AI-generated answer based on the ground answer and the context provided.\n",
    "Please respond with reasoning. Give some metrics to evaluate the answer and its completeness.\n",
    "Return your response in the following JSON format:\n",
    "{{\n",
    "    \"is_correct\": true/false,\n",
    "    \"reasoning\": \"Your reasoning here\",\n",
    "    \"metrics\": {{\n",
    "        \"relevance\": 0-1,\n",
    "        \"completeness\": 0-1,\n",
    "        \"accuracy\": 0-1\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "# Evaluate each question\n",
    "for i in range(l):\n",
    "    # q_key = f\"Q{i}\"\n",
    "    prompt = create_judge_prompt(\n",
    "        questions[i],\n",
    "        ground_answers[i],\n",
    "        generated_answers[i],\n",
    "        contexts[i]\n",
    "    )\n",
    "    print(f\"\\nEvaluating Question {i}...\\n\")\n",
    "    response = llm.call(prompt)\n",
    "    # result = llm.call(prompt)\n",
    "    print(response)\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(f\"Question {questions[i]}:\\n\")\n",
    "        file.write(f\"Response: {response}\\n\\n\")\n",
    "        print(f\"Response for Question {i} written to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1ed88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
